{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InPKrBOVi738"
      },
      "source": [
        "# Claude-3.5-Sonnet Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hardware - CPU"
      ],
      "metadata": {
        "id": "tKwnjVTvP1G2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations\n",
        "\n"
      ],
      "metadata": {
        "id": "v-a0BJY7YK4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecouSjl4K1M5",
        "outputId": "cfb33fd2-1e68-40f6-ac1c-cd8803763cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.34.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from anthropic)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from anthropic)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.20.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
            "Downloading anthropic-0.34.2-py3-none-any.whl (891 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.9/891.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, anthropic\n",
            "Successfully installed anthropic-0.34.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OplyZgWdjDcg"
      },
      "source": [
        "#00 - Google Drive Mount\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2OmS8YJjFk4",
        "outputId": "89f63b23-92f0-48f9-f364-3dce2c94ab7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwH3FsWijMf1"
      },
      "source": [
        "# 01 - Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD4wmq3sjM3t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import anthropic\n",
        "import httpx\n",
        "\n",
        "\n",
        "\n",
        "from time import sleep\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "from typing import Optional, Dict, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8B0_VExjARi"
      },
      "source": [
        "# 02 - Constants\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT2WF5XNjjrn"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'claude-3-5-sonnet'\n",
        "\n",
        "PREPROCESSED_DATA_ROOT_PATH = userdata.get('IA_DATA_PREPROCESSED')\n",
        "STAGING_AREA_ROOT_PATH = userdata.get('IA_DATA_STAGING')\n",
        "\n",
        "\n",
        "TEST_DATASETS = {Path(dataset).stem.split(\"_test\")[0]: {\"df\": pd.read_csv(dataset), \"path\":Path(dataset).parent} for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_test.csv')}\n",
        "DEMO_DATASETS = {Path(dataset).stem.split(\"_demo\")[0]: {\"df\": pd.read_csv(dataset), \"path\":Path(dataset).parent} for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_demo.csv')}\n",
        "RESULT_DATASETS = [Path(dataset).stem.split(f\"_{MODEL_NAME}_result_v2\")[0] for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_{MODEL_NAME}_result_v2.csv')]\n",
        "\n",
        "CLAUDE_API_KEY=userdata.get('CLAUDE_API_KEY')\n",
        "\n",
        "\n",
        "BASE_INSTRUCTION = \"\"\"Você deverá realizar a tarefa de Classificação de Sentimento Binária em relação a polaridade de textos escritos no idioma português brasileiro considerando dois possíveis rótulos de saída: 1 para o sentimentos positivos ou -1 para negativos. A saída produzida deverá ser em formato JSON, seguindo o esquema definido entre os marcadores ```.```\n",
        "{'type': 'object','description': Objeto de saída fornecido pelo classificador após a classificação de sentimento do texto de entrada.', 'properties': {'polaridade': {'type': 'integer','description': 'Polaridade em relação ao sentimento expressado no texto de entrada. Pode assumir 2 valores: [-1, 1]','enum': [-1,1]}},\n",
        "  'required': ['polaridade']}```Considere os seguintes exemplos para realizar a predição:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_nSlpYsdD39"
      },
      "source": [
        "# 03 - Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbzfUHOCdGvd"
      },
      "outputs": [],
      "source": [
        "def generate_sorted_examples(dataframe:pd.DataFrame)->str:\n",
        "    \"\"\"\n",
        "    Generate a string of sorted examples from a DataFrame for sentiment analysis.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): The DataFrame containing the examples.\n",
        "                                  It should have columns 'text' and 'label'.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing formatted examples of input text and their corresponding polarities.\n",
        "    \"\"\"\n",
        "    examples = ''\n",
        "    for i, _ in enumerate(dataframe[:3].index):\n",
        "        examples = examples + (\n",
        "        \"\\n\"\n",
        "        f\"Exemplo:\\n\"\n",
        "        f\"'entrada': '{dataframe['text'][i]}'\\n\"\n",
        "        \"'saida':{'polaridade': \" + f\"{dataframe['label'][i]}\"+\"}\"\n",
        "        \"\\n\"\n",
        "        f\"Exemplo:\\n\"\n",
        "        f\"'entrada': '{dataframe['text'][i+3]}'\\n\"\n",
        "        \"'saida':{'polaridade': \" + f\"{dataframe['label'][i+3]}\"+\"}\")\n",
        "    return examples\n",
        "\n",
        "\n",
        "def generate_classification_text(text:str)->str:\n",
        "    \"\"\"\n",
        "    Generate a formatted string for sentiment classification input and output.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be classified.\n",
        "\n",
        "    Returns:\n",
        "        str: A string formatted for sentiment classification showing the input text.\n",
        "    \"\"\"\n",
        "    classification = (\n",
        "        \"\\n\"\n",
        "        f\"Classificação de Sentimento:\"\n",
        "        f\"'entrada': '{text}'\"\n",
        "        \"'saida':\")\n",
        "    return classification\n",
        "\n",
        "\n",
        "def generate_staging_area_csv_file_name(model_name:str, dataset_name:str, fist_index:int, last_index:int)->str:\n",
        "    \"\"\"\n",
        "    Generate a staging area file name for a specific model and dataset.\n",
        "    \"\"\"\n",
        "    file_name = f'{dataset_name}_{model_name}_{fist_index}-{last_index}_v2.csv'\n",
        "    return file_name\n",
        "\n",
        "def get_staging_area_files_by_model_or_dataset_name(model_name:str=None, dataset_name:str=None)->list[Path]:\n",
        "    \"\"\"\n",
        "    Get a sorted list of staging area files for a specific model and/or dataset.\n",
        "    \"\"\"\n",
        "    if model_name != None and dataset_name != None:\n",
        "        files =  [Path(csv_file) for csv_file in glob.glob(f'{STAGING_AREA_ROOT_PATH}/{dataset_name}_{model_name}_*.csv')]\n",
        "    elif model_name != None:\n",
        "        files =  [Path(csv_file) for csv_file in glob.glob(f'{STAGING_AREA_ROOT_PATH}/*_{model_name}_*.csv')]\n",
        "    elif dataset_name != None:\n",
        "        files =  [Path(csv_file) for csv_file in glob.glob(f'{STAGING_AREA_ROOT_PATH}/{dataset_name}_*.csv')]\n",
        "    else:\n",
        "        raise ValueError('Either model_name or dataset_name must be provided.')\n",
        "    return sorted(files, key=lambda x: int(((str(x.absolute()).split('-')[-1]).split('.')[0]).split(\"_v\")[0]))\n",
        "\n",
        "def get_last_staging_area_file_content_index(model_name:str=None, dataset_name:str=None)->int:\n",
        "    \"\"\"\n",
        "    Get the index of the last staging area file for a specific model and dataset.\n",
        "    \"\"\"\n",
        "    if model_name and dataset_name:\n",
        "        files = get_staging_area_files_by_model_or_dataset_name(model_name=model_name, dataset_name=dataset_name)\n",
        "\n",
        "    elif model_name:\n",
        "        files = get_staging_area_files_by_model_or_dataset_name(model_name=model_name)\n",
        "\n",
        "    else:\n",
        "        files = get_staging_area_files_by_model_or_dataset_name(dataset_name=dataset_name)\n",
        "    if len(files) == 0:\n",
        "        return None\n",
        "\n",
        "    last_file = str(files[-1].absolute())\n",
        "    last_file_index_name = int((last_file.split('_')[-2]).split('.')[0].split('-')[-1])\n",
        "    return last_file_index_name\n",
        "\n",
        "def save_csv_to_staging_area(model_name:str, dataset_name:str, results_list:list)->Path:\n",
        "    \"\"\"\n",
        "    Save a CSV file to the staging area.\n",
        "    \"\"\"\n",
        "    sorted_result_list = sorted(results_list, key=lambda x: x[0])\n",
        "    first_index = sorted_result_list[0][0]\n",
        "    last_index = sorted_result_list[-1][0]\n",
        "    file_name = generate_staging_area_csv_file_name(\n",
        "        model_name=model_name,\n",
        "        dataset_name=dataset_name,\n",
        "        fist_index=first_index,\n",
        "        last_index=last_index\n",
        "        )\n",
        "\n",
        "    file_path = f'{STAGING_AREA_ROOT_PATH}{file_name}'\n",
        "    df = pd.DataFrame(results_list, columns=['index','predictions','inferece_time', 'input_tokens', 'output_tokens'])\n",
        "    df.to_csv(file_path)\n",
        "    print(f'File containing index {first_index} to {last_index} for dataset {dataset_name} and model {model_name} created at {file_path} with success.')\n",
        "    return Path(file_path +\".csv\")\n",
        "\n",
        "\n",
        "def convert_csv_in_staging_area_to_dataframe(list_of_files:list[Path])->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a list of CSV files in the staging area to a single DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame(columns=[\"Unmatch: 0\", \"index\", \"predictions\", \"inferece_time\", \"input_tokens\", \"output_tokens\"])\n",
        "    for file in list_of_files:\n",
        "        temp_df = pd.read_csv(file)\n",
        "        df = pd.concat([df, temp_df])\n",
        "\n",
        "    df = df.sort_values(by='index')\n",
        "    df = df.drop_duplicates(subset=['index'])\n",
        "    df = df.reset_index(drop=True).set_index('index')\n",
        "    df = df[['predictions']]\n",
        "    return df\n",
        "\n",
        "class HeaderCapturingClient(httpx.Client):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.last_headers = None\n",
        "\n",
        "    def send(self, request, *args, **kwargs):\n",
        "        response = super().send(request, *args, **kwargs)\n",
        "        self.last_headers = response.headers\n",
        "        return response\n",
        "\n",
        "\n",
        "def make_api_call(client: anthropic.Anthropic, params: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    return client.messages.create(**params)\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
        "    retry=retry_if_exception_type((anthropic.InternalServerError)),\n",
        "    reraise=True\n",
        ")\n",
        "def make_prediction(\n",
        "    client: anthropic.Anthropic,\n",
        "    system_instruction: str,\n",
        "    classification_text: str,\n",
        "    max_tokens: int = 20,\n",
        "    model: str = 'claude-3-5-sonnet-20240620',\n",
        "    temperature: float = 0.0\n",
        ") -> Optional[str]:\n",
        "    try:\n",
        "        params = {\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"model\": model,\n",
        "            \"temperature\": temperature,\n",
        "            \"system\": system_instruction,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": classification_text},\n",
        "                {\"role\": \"assistant\", \"content\": \"json\"}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        message = make_api_call(client, params)\n",
        "        return message\n",
        "\n",
        "    except anthropic.RateLimitError as e:\n",
        "        print(f\"Rate limit exceeded: {e}\")\n",
        "        raise\n",
        "\n",
        "    except anthropic.APIStatusError as e:\n",
        "        print(f\"API Status Error: {e.status_code} - {e.response}\")\n",
        "        raise\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        raise\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_response_headers_limits(response_headers: Dict[str, str]) -> Dict[str, str]:\n",
        "    rate_limits = {\n",
        "        'ratelimit-requests-limit': response_headers.get('anthropic-ratelimit-requests-limit'),\n",
        "        'ratelimit-requests-reset': response_headers.get('anthropic-ratelimit-requests-reset'),\n",
        "        'ratelimit-requests-remaining': response_headers.get('anthropic-ratelimit-requests-remaining'),\n",
        "        'ratelimit-tokens-limit': response_headers.get('anthropic-ratelimit-tokens-limit'),\n",
        "        'ratelimit-tokens-remaining': response_headers.get('anthropic-ratelimit-tokens-remaining'),\n",
        "        'ratelimit-tokens-reset': response_headers.get('anthropic-ratelimit-tokens-reset')\n",
        "    }\n",
        "\n",
        "    return rate_limits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr4Hm9fGutMY"
      },
      "source": [
        "# 04 - Execution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_http_client = HeaderCapturingClient()\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=CLAUDE_API_KEY,\n",
        "    http_client=custom_http_client\n",
        "    )"
      ],
      "metadata": {
        "id": "bm0XqMBMY4Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqRaWGIxsmMt"
      },
      "outputs": [],
      "source": [
        "for key, dataset in TEST_DATASETS.items():\n",
        "    if key not in RESULT_DATASETS:\n",
        "        print(f'Starting to evaluate dataset: {key}')\n",
        "        results_list = []\n",
        "        examples = generate_sorted_examples(DEMO_DATASETS[key]['df'])\n",
        "        instruction = BASE_INSTRUCTION + examples\n",
        "\n",
        "        first_index = get_last_staging_area_file_content_index(dataset_name=key, model_name=MODEL_NAME)\n",
        "        if first_index == None:\n",
        "            first_index = 0\n",
        "            print(f'No file were found for the current dataset and model, starting from index 0')\n",
        "\n",
        "        else:\n",
        "            first_index += 1\n",
        "            print(f'One or more files were found. Starting from lastest index found ({first_index}) for dataset {key}')\n",
        "\n",
        "        for index, item in dataset['df']['text'][first_index:].items():\n",
        "\n",
        "\n",
        "            classification_text = generate_classification_text(item)\n",
        "\n",
        "\n",
        "            prediction = make_prediction(client=client,\n",
        "                                         system_instruction=instruction,\n",
        "                                         classification_text=classification_text)\n",
        "\n",
        "            rate_limits = parse_response_headers_limits(custom_http_client.last_headers)\n",
        "            print(rate_limits)\n",
        "            if prediction is None:\n",
        "                raise Exception('Prediction is None')\n",
        "\n",
        "            results_list.append((index, prediction.content[0].text))\n",
        "            print((index, prediction.content[0].text))\n",
        "            sleep(1.5)\n",
        "\n",
        "\n",
        "            if len(results_list) % 100 == 0:\n",
        "                save_csv_to_staging_area(model_name=MODEL_NAME,\n",
        "                                         dataset_name=key,\n",
        "                                         results_list=results_list)\n",
        "\n",
        "\n",
        "        save_csv_to_staging_area(model_name=MODEL_NAME,\n",
        "                                 dataset_name=key,\n",
        "                                 results_list=results_list)\n",
        "\n",
        "\n",
        "\n",
        "        if len(results_list) != (dataset['df'].shape[0]):\n",
        "            df = convert_csv_in_staging_area_to_dataframe(get_staging_area_files_by_model_or_dataset_name(dataset_name=key, model_name=MODEL_NAME))\n",
        "            if df.shape[0] == dataset['df'].shape[0]:\n",
        "                dataset['df']['predictions'] = df['predictions']\n",
        "\n",
        "\n",
        "        elif len(results_list) == (dataset['df'].shape[0]):\n",
        "            dataset['df']['predictions'] = np.array(results_list)[:,1]\n",
        "\n",
        "        else:\n",
        "            raise ValueError('The number of results is different from the number of rows in the dataset.')\n",
        "\n",
        "        dataset['df'].to_csv(f'{str(dataset[\"path\"])}/{key}_{MODEL_NAME}_result.csv', index=False)\n",
        "        RESULT_DATASETS = [Path(dataset).stem.split(f\"_{MODEL_NAME}_result_v2\")[0] for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_{MODEL_NAME}_result_v2.csv')]\n",
        "        print(f'The evaluation of dataset:{key} has ended.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}