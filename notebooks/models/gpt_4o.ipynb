{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InPKrBOVi738"
      },
      "source": [
        "# GPT-4o Evaluation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hardware - CPU"
      ],
      "metadata": {
        "id": "cT9w_Tn5RDqw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5f2Ei3Ei-3d"
      },
      "source": [
        "# Installations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLzGHu3VKdyh",
        "outputId": "5205124c-8375-4785-ce9d-08a98db9daa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OplyZgWdjDcg"
      },
      "source": [
        "#00 - Google Drive Mount\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2OmS8YJjFk4",
        "outputId": "fa20e6ae-5109-4f06-b3b0-95977c56fc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwH3FsWijMf1"
      },
      "source": [
        "# 01 - Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD4wmq3sjM3t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import openai\n",
        "import tiktoken\n",
        "\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8B0_VExjARi"
      },
      "source": [
        "# 02 - Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT2WF5XNjjrn"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gtp-4o\"\n",
        "\n",
        "PREPROCESSED_DATA_ROOT_PATH = userdata.get('IA_DATA_PREPROCESSED')\n",
        "STAGING_AREA_ROOT_PATH = userdata.get('IA_DATA_STAGING')\n",
        "\n",
        "TEST_DATASETS = {Path(dataset).stem.split(\"_test\")[0]: {\"df\": pd.read_csv(dataset), \"path\":Path(dataset).parent} for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_test.csv')}\n",
        "DEMO_DATASETS = {Path(dataset).stem.split(\"_demo\")[0]: {\"df\": pd.read_csv(dataset), \"path\":Path(dataset).parent} for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_demo.csv')}\n",
        "RESULT_DATASETS = [Path(dataset).stem.split(f\"_{MODEL_NAME}_result_v2\")[0] for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_{MODEL_NAME}_result_v2.csv')]\n",
        "\n",
        "OPENAI_AUTH = userdata.get('OPENAI_TOKEN')\n",
        "ORGANIZATION_ID = userdata.get(\"OPENAI_ORGANIZATION_ID\")\n",
        "PROJECT_ID = userdata.get(\"OPENAI_PROJECT_ID\")\n",
        "\n",
        "BASE_INSTRUCTION = \"\"\"Você deverá realizar a tarefa de Classificação de Sentimento Binária em relação a polaridade de textos escritos no idioma português brasileiro considerando dois possíveis rótulos de saída: 1 para o sentimentos positivos ou -1 para negativos. A saída produzida deverá ser em formato JSON, seguindo o esquema definido entre os marcadores ```.```\n",
        "{'type': 'object','description': Objeto de saída fornecido pelo classificador após a classificação de sentimento do texto de entrada.', 'properties': {'polaridade': {'type': 'integer','description': 'Polaridade em relação ao sentimento expressado no texto de entrada. Pode assumir 2 valores: [-1, 1]','enum': [-1,1]}},\n",
        "  'required': ['polaridade']}```Considere os seguintes exemplos para realizar a predição:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH7rdn-nvORF"
      },
      "source": [
        "# 03 - Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "733srlwQvOYH"
      },
      "outputs": [],
      "source": [
        "def generate_sorted_examples(dataframe:pd.DataFrame)->str:\n",
        "    \"\"\"\n",
        "    Generate a string of sorted examples from a DataFrame for sentiment analysis.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): The DataFrame containing the examples.\n",
        "                                  It should have columns 'text' and 'label'.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing formatted examples of input text and their corresponding polarities.\n",
        "    \"\"\"\n",
        "    examples = ''\n",
        "    for i, _ in enumerate(dataframe[:3].index):\n",
        "        examples = examples + (\n",
        "        \"\\n\"\n",
        "        f\"Exemplo:\\n\"\n",
        "        f\"'entrada': '{dataframe['text'][i]}'\\n\"\n",
        "        \"'saida':{'polaridade': \" + f\"{dataframe['label'][i]}\"+\"}\"\n",
        "        \"\\n\"\n",
        "        f\"Exemplo:\\n\"\n",
        "        f\"'entrada': '{dataframe['text'][i+3]}'\\n\"\n",
        "        \"'saida':{'polaridade': \" + f\"{dataframe['label'][i+3]}\"+\"}\")\n",
        "    return examples\n",
        "\n",
        "\n",
        "def generate_classification_text(text:str)->str:\n",
        "    \"\"\"\n",
        "    Generate a formatted string for sentiment classification input and output.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be classified.\n",
        "\n",
        "    Returns:\n",
        "        str: A string formatted for sentiment classification showing the input text.\n",
        "    \"\"\"\n",
        "    classification = (\n",
        "        \"\\n\"\n",
        "        f\"Classificação de Sentimento:\"\n",
        "        f\"'entrada': '{text}'\"\n",
        "        \"'saida':\")\n",
        "    return classification\n",
        "\n",
        "\n",
        "#Open IA Batch management\n",
        "\n",
        "def upload_batch_file(open_ia_client:openai.OpenAI, file_path:str|Path)->openai.types.file_object.FileObject:\n",
        "    \"\"\"\n",
        "    Upload a batch file to the OpenAI client.\n",
        "\n",
        "    Args:\n",
        "        open_ia_client (openai.OpenAI): The OpenAI client instance.\n",
        "        file_path (str | Path): The path to the file to be uploaded.\n",
        "\n",
        "    Returns:\n",
        "        openai.types.file_object.FileObject: The uploaded file object.\n",
        "    \"\"\"\n",
        "    if isinstance(file_path, str):\n",
        "        file_path = Path(file_path)\n",
        "    if file_path.exists():\n",
        "        batch_input_file = open_ia_client.files.create(\n",
        "            file=open(file_path, \"rb\"),\n",
        "            purpose=\"batch\")\n",
        "        return batch_input_file\n",
        "\n",
        "def create_batch(open_ia_client:openai.OpenAI, batch_input_file:openai.types.file_object.FileObject)->openai.types.batch.Batch:\n",
        "    \"\"\"\n",
        "    Create a batch from an uploaded file.\n",
        "\n",
        "    Args:\n",
        "        open_ia_client (openai.OpenAI): The OpenAI client instance.\n",
        "        batch_input_file (openai.types.file_object.FileObject): The uploaded file object.\n",
        "\n",
        "    Returns:\n",
        "        openai.types.batch.Batch: The created batch object.\n",
        "    \"\"\"\n",
        "    if isinstance(batch_input_file, openai.types.file_object.FileObject):\n",
        "        batch = client.batches.create(\n",
        "            input_file_id=batch_input_file.id,\n",
        "            endpoint=\"/v1/chat/completions\",\n",
        "            completion_window=\"24h\")\n",
        "        return batch\n",
        "\n",
        "def retrive_batch(open_ia_client:openai.OpenAI, batch: openai.types.batch.Batch)->str:\n",
        "    \"\"\"\n",
        "    Retrieve the details of a batch.\n",
        "\n",
        "    Args:\n",
        "        open_ia_client (openai.OpenAI): The OpenAI client instance.\n",
        "        batch (openai.types.batch.Batch): The batch object to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        str: The details of the batch.\n",
        "    \"\"\"\n",
        "    if isinstance(batch, openai.types.batch.Batch):\n",
        "        batch = open_ia_client.batches.retrieve(batch.id)\n",
        "        return batch\n",
        "\n",
        "def get_batches(open_ia_client:openai.OpenAI, limit:int=15)->list:\n",
        "    \"\"\"\n",
        "    Retrieve a list of batches with a specified limit.\n",
        "\n",
        "    Args:\n",
        "        open_ia_client (openai.OpenAI): The OpenAI client instance.\n",
        "        limit (int, optional): The maximum number of batches to retrieve. Defaults to 15.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of batch data.\n",
        "    \"\"\"\n",
        "    batches = client.batches.list(limit=limit)\n",
        "    batches_data = batches.data\n",
        "    if batches.has_more==True:\n",
        "        while batches.has_more==True:\n",
        "            batches = client.batches.list(limit=limit, after=batches.last_id)\n",
        "            batches_data.extend(batches.data)\n",
        "    return batches_data\n",
        "\n",
        "def get_batch_status(batch:openai.types.batch.Batch)->str:\n",
        "    \"\"\"\n",
        "    Get the status of a batch.\n",
        "\n",
        "    Args:\n",
        "        batch (openai.types.batch.Batch): The batch object.\n",
        "\n",
        "    Returns:\n",
        "        str: The status of the batch.\n",
        "    \"\"\"\n",
        "    if isinstance(batch, openai.types.batch.Batch):\n",
        "        return batch.status\n",
        "\n",
        "def get_batch_output_file_id(batch:openai.types.batch.Batch)->str:\n",
        "    \"\"\"\n",
        "    Get the output file ID of a batch.\n",
        "\n",
        "    Args:\n",
        "        batch (openai.types.batch.Batch): The batch object.\n",
        "\n",
        "    Returns:\n",
        "        str: The output file ID of the batch.\n",
        "    \"\"\"\n",
        "    if isinstance(batch, openai.types.batch.Batch):\n",
        "        return batch.output_file_id\n",
        "\n",
        "def get_file_content(file_id:str, open_ia_client:openai.OpenAI)->str:\n",
        "    \"\"\"\n",
        "    Get the content of a file by its ID.\n",
        "\n",
        "    Args:\n",
        "        file_id (str): The ID of the file.\n",
        "        open_ia_client (openai.OpenAI): The OpenAI client instance.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the file.\n",
        "    \"\"\"\n",
        "    content = open_ia_client.files.content(file_id)\n",
        "    content = content.text.split(\"\\n\")\n",
        "    content = [json.loads(line) for line in content if line != '']\n",
        "    return content\n",
        "\n",
        "def get_response_text(response:json)->str:\n",
        "    \"\"\"\n",
        "    Get the response text from a response dictionary.\n",
        "\n",
        "    Args:\n",
        "        response (dict): The response dictionary.\n",
        "\n",
        "    Returns:\n",
        "        str: The response text.\n",
        "    \"\"\"\n",
        "    if isinstance(response, dict):\n",
        "        return response['response']['body']['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_response_usage(response:json)->str:\n",
        "    \"\"\"\n",
        "    Get the response usage from a response dictionary.\n",
        "\n",
        "    Args:\n",
        "        response (dict): The response dictionary.\n",
        "\n",
        "    Returns:\n",
        "        str: The response usage.\n",
        "    \"\"\"\n",
        "    if isinstance(response, dict):\n",
        "        return response['response']['body']['usage']\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqsp1CelvUMT"
      },
      "source": [
        "# 04 - Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Td6BS3g-UC8"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "  organization=ORGANIZATION_ID,\n",
        "  project=PROJECT_ID,\n",
        "  api_key = OPENAI_AUTH\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUEBG7RRGsOn"
      },
      "source": [
        "## Batches\n",
        "\n",
        "In order to reduce the cost of API calls, we will use [OpenAI's batch API](https://platform.openai.com/docs/guides/batch). The first stage consist in creating the batch files for each dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbKtw9YR4Szm"
      },
      "source": [
        "### Genearting batch files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iPEgB--u9XFo"
      },
      "outputs": [],
      "source": [
        "for key, dataset in TEST_DATASETS.items():\n",
        "    if key not in RESULT_DATASETS:\n",
        "        examples = generate_sorted_examples(DEMO_DATASETS[key]['df'])\n",
        "        instruction = BASE_INSTRUCTION + examples\n",
        "        batch_list = []\n",
        "        n_tokens = 0\n",
        "        batch_number = 1\n",
        "\n",
        "        for index, item in enumerate(dataset['df']['text']):\n",
        "            classification_text = generate_classification_text(item)\n",
        "            n_tokens += len(tiktoken.encoding_for_model(\"gpt-4o\").encode(instruction + classification_text))\n",
        "\n",
        "            batch = {\"custom_id\":f\"request_{key}_{index}\",\n",
        "                     \"method\": \"POST\",\n",
        "                     \"url\": \"/v1/chat/completions\",\n",
        "                     \"body\": {\n",
        "                          \"model\": \"gpt-4o\",\n",
        "                          \"messages\": [\n",
        "                              {\"role\": \"system\", \"content\": instruction},\n",
        "                               {\"role\":\"user\", \"content\":classification_text}\n",
        "                              ],\n",
        "                          \"max_tokens\":20,\n",
        "                          \"n\":1,\n",
        "                          \"seed\":4,\n",
        "                          \"temperature\":0,\n",
        "                          \"response_format\":{\"type\": \"json_object\"}\n",
        "                          }\n",
        "                     }\n",
        "\n",
        "            batch_list.append(batch)\n",
        "\n",
        "        if n_tokens < 80_000:\n",
        "            with open(f'{key}_batch{batch_number}_list.jsonl', 'w') as f:\n",
        "                for item in batch_list:\n",
        "                    f.write(json.dumps(item) + \"\\n\")\n",
        "        else:\n",
        "            temp_batch_list = batch_list.copy()\n",
        "            while len(temp_batch_list) > 0:\n",
        "                n_tokens = 0\n",
        "                temp_dict = {}\n",
        "                temp_dict[f\"batch-{batch_number}\"] = []\n",
        "                while n_tokens < 80_000:\n",
        "                    for batch in temp_batch_list:\n",
        "                        text = batch['body']['messages'][0]['content']+batch['body']['messages'][1]['content']\n",
        "                        current_tokens = len(tiktoken.encoding_for_model(\"gpt-4o\").encode(text))\n",
        "                        if n_tokens + current_tokens < 80_000:\n",
        "                            temp_dict[f\"batch-{batch_number}\"].append(batch) if batch not in temp_dict[f\"batch-{batch_number}\"] else None\n",
        "                            n_tokens += current_tokens\n",
        "                        else:\n",
        "                            n_tokens = 80_000\n",
        "                            break\n",
        "\n",
        "                _ = [temp_batch_list.remove(batch) for batch in temp_dict[f\"batch-{batch_number}\"]]\n",
        "\n",
        "                if not os.path.exists(f'content/batches/{key}'):\n",
        "                    os.makedirs(f'content/batches/{key}')\n",
        "\n",
        "                with open(f'content/batches/{key}/{key}_batch-{\"0\" + str(batch_number) if batch_number <10 else batch_number}_list.jsonl', 'w') as f:\n",
        "                    for item in temp_dict[f\"batch-{batch_number}\"]:\n",
        "                        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "                batch_number += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After all files were created, they are uploaded through the API."
      ],
      "metadata": {
        "id": "D07lcMa_C_Yb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnF6KQcZ4OFH"
      },
      "source": [
        "### Upload batch files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECpaCxPE1qt7"
      },
      "outputs": [],
      "source": [
        "jsonl_paths = {}\n",
        "\n",
        "for path in Path('/content/content/batches').rglob('*.jsonl'):\n",
        "    file_name = path.stem\n",
        "    key = file_name.split(\"_batch\")[0]\n",
        "    if key not in jsonl_paths.keys():\n",
        "        jsonl_paths[key] = []\n",
        "        jsonl_paths[key].append(path)\n",
        "    else:\n",
        "        jsonl_paths[key].append(path)\n",
        "\n",
        "jsonl_paths = {key: sorted(jsonl_paths[key]) for key in jsonl_paths.keys()}\n",
        "\n",
        "for item in jsonl_paths['imdb-pt']:\n",
        "    batch_input_file = upload_batch_file(client, item)\n",
        "    batch = create_batch(client, batch_input_file)\n",
        "    while batch.status in ['validating', 'in_progress', 'finalizing']:\n",
        "        sleep(60)\n",
        "        batch = retrive_batch(client, batch)\n",
        "    if batch.status != 'completed':\n",
        "        sleep(120)\n",
        "        batch = create_batch(client, batch_input_file)\n",
        "        while batch.status in ['validating', 'in_progress', 'finalizing']:\n",
        "            sleep(60)\n",
        "            batch = retrive_batch(client, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the already processed batches are retrieved and sorted according to their original positions"
      ],
      "metadata": {
        "id": "4J_FUXE8DGwS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW5mRiYvnDrQ"
      },
      "source": [
        "### Retrive batch results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrfgNoe6HJkE"
      },
      "outputs": [],
      "source": [
        "batches = get_batches(open_ia_client=client, limit=100)\n",
        "batches_completed = [(get_batch_status(batch), get_batch_output_file_id(batch)) for batch in batches if get_batch_status(batch) == \"completed\"]\n",
        "batch_contents = [get_file_content(batch_status[1], client) for batch_status in batches_completed]\n",
        "\n",
        "results = {}\n",
        "for responses in batch_contents:\n",
        "    text = []\n",
        "    usage = []\n",
        "    id = []\n",
        "    for response in responses:\n",
        "        key = response['custom_id'].split(\"_\")[1]\n",
        "        text.append(get_response_text(response))\n",
        "        usage.append(get_response_usage(response))\n",
        "        id.append(int(response['custom_id'].split('_')[-1]))\n",
        "\n",
        "    if results.get(key) is None:\n",
        "        results[key] = {}\n",
        "        results[key]['text'] = text\n",
        "        results[key]['usage'] = usage\n",
        "        results[key]['id'] = id\n",
        "\n",
        "    else:\n",
        "        results[key]['text'].extend(text)\n",
        "        results[key]['usage'].extend(usage)\n",
        "        results[key]['id'].extend(id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, the results are organized and persisted as CSV files."
      ],
      "metadata": {
        "id": "mKRfm0eIDTWb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG0ZOKmonOca"
      },
      "source": [
        "### Persist batches results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ_YJL5ogxFM",
        "outputId": "ec960dae-2d4f-435d-db30-af12e2b5024f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key: imdb-pt\n",
            "imdb-pt Processed with success\n"
          ]
        }
      ],
      "source": [
        "for key in results.keys():\n",
        "    RESULT_DATASETS = [Path(dataset).stem.split(f\"_{MODEL_NAME}_result_v2\")[0] for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_{MODEL_NAME}_result_v2.csv')]\n",
        "    if key not in RESULT_DATASETS:\n",
        "        print(f'Processing key: {key}')\n",
        "        if results[key].get('id'):\n",
        "            df = pd.DataFrame(results[key], index=results[key]['id'])\n",
        "            df = df.drop_duplicates(['id'])\n",
        "            df = df.sort_index()\n",
        "        else:\n",
        "            break\n",
        "        if TEST_DATASETS[key]['df'].shape[0] != df.shape[0]:\n",
        "            print(f'Datasets do not have same shape')\n",
        "            break\n",
        "\n",
        "        TEST_DATASETS[key]['df']['predictions'] = df['text']\n",
        "        TEST_DATASETS[key]['df']['usage'] = df['usage']\n",
        "        TEST_DATASETS[key]['df'].to_csv(f'{str(TEST_DATASETS[key][\"path\"])}/{key}_{MODEL_NAME}_result.csv', index=False)\n",
        "        print(f'{key} Processed with success')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}