{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DeepSeek-V3 Evaluation\n"
      ],
      "metadata": {
        "id": "InPKrBOVi738"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hardware - CPU"
      ],
      "metadata": {
        "id": "kBMP8VhEQaQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations\n",
        "\n"
      ],
      "metadata": {
        "id": "a5f2Ei3Ei-3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#00 - Google Drive Mount\n",
        "\n"
      ],
      "metadata": {
        "id": "OplyZgWdjDcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2OmS8YJjFk4",
        "outputId": "141dd23c-45a1-4a8c-80b7-7264fa44b87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01 - Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "MwH3FsWijMf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "\n",
        "from time import sleep\n",
        "from typing_extensions import Counter"
      ],
      "metadata": {
        "id": "HD4wmq3sjM3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02 - Constants\n",
        "\n"
      ],
      "metadata": {
        "id": "e8B0_VExjARi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME  = \"deepseek-v3-671B\"\n",
        "\n",
        "PREPROCESSED_DATA_ROOT_PATH = userdata.get(\"IA_DATA_PREPROCESSED\")\n",
        "STAGING_AREA_ROOT_PATH = userdata.get(\"IA_DATA_STAGING\")\n",
        "\n",
        "\n",
        "TEST_DATASETS = {Path(dataset).stem.split(\"_test\")[0]: {\"df\": pd.read_csv(dataset), \"path\":Path(dataset).parent} for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_test.csv')}\n",
        "DEMO_DATASETS = {Path(dataset).stem.split(\"_demo\")[0]: {\"df\": pd.read_csv(dataset), \"path\":Path(dataset).parent} for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_demo.csv')}\n",
        "RESULT_DATASETS = [Path(dataset).stem.split(f\"_{MODEL_NAME}_result_v2\")[0] for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_{MODEL_NAME}_result_v2.csv')]\n",
        "\n",
        "AUTH = userdata.get(\"OPEN_ROUTER_TOKEN\")\n",
        "\n",
        "BASE_INSTRUCTION = \"\"\"Você deverá realizar a tarefa de Classificação de Sentimento Binária em relação a polaridade de textos escritos no idioma português brasileiro considerando dois possíveis rótulos de saída: 1 para o sentimentos positivos ou -1 para negativos. A saída produzida deverá ser em formato JSON, seguindo o esquema definido entre os marcadores ```.```\n",
        "{'type': 'object','description': Objeto de saída fornecido pelo classificador após a classificação de sentimento do texto de entrada.', 'properties': {'polaridade': {'type': 'integer','description': 'Polaridade em relação ao sentimento expressado no texto de entrada. Pode assumir 2 valores: [-1, 1]','enum': [-1,1]}},\n",
        "  'required': ['polaridade']}```Considere os seguintes exemplos para realizar a predição:\"\"\""
      ],
      "metadata": {
        "id": "FT2WF5XNjjrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03 - Functions\n"
      ],
      "metadata": {
        "id": "vH7rdn-nvORF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sorted_examples(dataframe:pd.DataFrame)->str:\n",
        "    \"\"\"\n",
        "    Generate a string of sorted examples from a DataFrame for sentiment analysis.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): The DataFrame containing the examples.\n",
        "                                  It should have columns 'text' and 'label'.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing formatted examples of input text and their corresponding polarities.\n",
        "    \"\"\"\n",
        "    examples = ''\n",
        "    for i, _ in enumerate(dataframe[:3].index):\n",
        "        examples = examples + (\n",
        "        \"\\n\"\n",
        "        f\"Exemplo:\\n\"\n",
        "        f\"'entrada': '{dataframe['text'][i]}'\\n\"\n",
        "        \"'saida':{'polaridade': \" + f\"{dataframe['label'][i]}\"+\"}\"\n",
        "        \"\\n\"\n",
        "        f\"Exemplo:\\n\"\n",
        "        f\"'entrada': '{dataframe['text'][i+3]}'\\n\"\n",
        "        \"'saida':{'polaridade': \" + f\"{dataframe['label'][i+3]}\"+\"}\")\n",
        "    return examples\n",
        "\n",
        "\n",
        "def generate_classification_text(text:str)->str:\n",
        "    \"\"\"\n",
        "    Generate a formatted string for sentiment classification input and output.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be classified.\n",
        "\n",
        "    Returns:\n",
        "        str: A string formatted for sentiment classification showing the input text.\n",
        "    \"\"\"\n",
        "    classification = (\n",
        "        \"\\n\"\n",
        "        f\"Classificação de Sentimento:\"\n",
        "        f\"'entrada': '{text}'\"\n",
        "        \"'saida':\")\n",
        "    return classification\n",
        "\n",
        "\n",
        "def generate_staging_area_csv_file_name(model_name:str, dataset_name:str, fist_index:int, last_index:int)->str:\n",
        "    \"\"\"\n",
        "    Generate a staging area file name for a specific model and dataset.\n",
        "    \"\"\"\n",
        "    file_name = f'{dataset_name}_{model_name}_{fist_index}-{last_index}_v2.csv'\n",
        "    return file_name\n",
        "\n",
        "def get_staging_area_files_by_model_or_dataset_name(model_name:str=None, dataset_name:str=None)->list[Path]:\n",
        "    \"\"\"\n",
        "    Get a sorted list of staging area files for a specific model and/or dataset.\n",
        "    \"\"\"\n",
        "    if model_name != None and dataset_name != None:\n",
        "        files =  [Path(csv_file) for csv_file in glob.glob(f'{STAGING_AREA_ROOT_PATH}/{dataset_name}_{model_name}_*.csv')]\n",
        "    elif model_name != None:\n",
        "        files =  [Path(csv_file) for csv_file in glob.glob(f'{STAGING_AREA_ROOT_PATH}/*_{model_name}_*.csv')]\n",
        "    elif dataset_name != None:\n",
        "        files =  [Path(csv_file) for csv_file in glob.glob(f'{STAGING_AREA_ROOT_PATH}/{dataset_name}_*.csv')]\n",
        "    else:\n",
        "        raise ValueError('Either model_name or dataset_name must be provided.')\n",
        "    return sorted(files, key=lambda x: int(((str(x.absolute()).split('-')[-1]).split('.')[0]).split(\"_v\")[0]))\n",
        "\n",
        "def get_last_staging_area_file_content_index(model_name:str=None, dataset_name:str=None)->int:\n",
        "    \"\"\"\n",
        "    Get the index of the last staging area file for a specific model and dataset.\n",
        "    \"\"\"\n",
        "    if model_name and dataset_name:\n",
        "        files = get_staging_area_files_by_model_or_dataset_name(model_name=model_name, dataset_name=dataset_name)\n",
        "\n",
        "    elif model_name:\n",
        "        files = get_staging_area_files_by_model_or_dataset_name(model_name=model_name)\n",
        "\n",
        "    else:\n",
        "        files = get_staging_area_files_by_model_or_dataset_name(dataset_name=dataset_name)\n",
        "    if len(files) == 0:\n",
        "        return None\n",
        "\n",
        "    last_file = str(files[-1].absolute())\n",
        "    last_file_index_name = int((last_file.split('_')[-2]).split('.')[0].split('-')[-1])\n",
        "    return last_file_index_name\n",
        "\n",
        "def save_csv_to_staging_area(model_name:str, dataset_name:str, results_list:list)->Path:\n",
        "    \"\"\"\n",
        "    Save a CSV file to the staging area.\n",
        "    \"\"\"\n",
        "    sorted_result_list = sorted(results_list, key=lambda x: x[0])\n",
        "    first_index = sorted_result_list[0][0]\n",
        "    last_index = sorted_result_list[-1][0]\n",
        "    file_name = generate_staging_area_csv_file_name(\n",
        "        model_name=model_name,\n",
        "        dataset_name=dataset_name,\n",
        "        fist_index=first_index,\n",
        "        last_index=last_index\n",
        "        )\n",
        "\n",
        "    file_path = f'{STAGING_AREA_ROOT_PATH}{file_name}'\n",
        "    df = pd.DataFrame(results_list, columns=['index','predictions','inferece_time', 'input_tokens', 'output_tokens'])\n",
        "    df.to_csv(file_path)\n",
        "    print(f'File containing index {first_index} to {last_index} for dataset {dataset_name} and model {model_name} created at {file_path} with success.')\n",
        "    return Path(file_path +\".csv\")\n",
        "\n",
        "\n",
        "def convert_csv_in_staging_area_to_dataframe(list_of_files:list[Path])->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a list of CSV files in the staging area to a single DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame(columns=[\"Unmatch: 0\", \"index\", \"predictions\", \"inferece_time\", \"input_tokens\", \"output_tokens\"])\n",
        "    for file in list_of_files:\n",
        "        temp_df = pd.read_csv(file)\n",
        "        df = pd.concat([df, temp_df])\n",
        "\n",
        "    df = df.sort_values(by='index')\n",
        "    df = df.drop_duplicates(subset=['index'])\n",
        "    df = df.reset_index(drop=True).set_index('index')\n",
        "    df = df[['predictions']]\n",
        "    return df"
      ],
      "metadata": {
        "id": "733srlwQvOYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04 - Execution"
      ],
      "metadata": {
        "id": "nqsp1CelvUMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, dataset in TEST_DATASETS.items():\n",
        "    if key not in RESULT_DATASETS:\n",
        "        print(f'Starting to evaluate dataset: {key}')\n",
        "        results_list = []\n",
        "        examples = generate_sorted_examples(DEMO_DATASETS[key]['df'])\n",
        "        instruction = BASE_INSTRUCTION + examples\n",
        "\n",
        "        first_index = get_last_staging_area_file_content_index(dataset_name=key, model_name=model_name)\n",
        "        if first_index == None:\n",
        "            first_index = 0\n",
        "            print(f'No file were found for the current dataset and model, starting from index 0')\n",
        "\n",
        "        else:\n",
        "            first_index += 1\n",
        "            print(f'One or more files were found. Starting from lastest index found ({first_index}) for dataset {key}')\n",
        "\n",
        "        for index, item in dataset['df']['text'][first_index:].items():\n",
        "            counter=0\n",
        "            finish_reason = None\n",
        "            classification_text = generate_classification_text(item)\n",
        "\n",
        "            while not finish_reason:\n",
        "                resp = make_classification_request(instruction=instruction, classification_text=classification_text)\n",
        "                finish_reason = resp.json().get(\"choices\")[0].get(\"finish_reason\")\n",
        "                provider = resp.json().get(\"provider\")\n",
        "                prediction = resp.json().get(\"choices\")[0].get(\"message\").get(\"content\")\n",
        "                reasoning = resp.json().get(\"choices\")[0].get(\"message\").get(\"reasoning\")\n",
        "                prompt_tokens = resp.json().get(\"usage\").get(\"prompt_tokens\")\n",
        "                completion_tokens = resp.json().get(\"usage\").get(\"completion_tokens\")\n",
        "                total_tokens = resp.json().get(\"usage\").get(\"total_tokens\")\n",
        "                sleep(1*counter)\n",
        "                counter += 1\n",
        "\n",
        "                if counter > 10:\n",
        "                    raise ValueError('Too many requests')\n",
        "\n",
        "            results_list.append((index, prediction, reasoning, provider, finish_reason, prompt_tokens, completion_tokens, total_tokens))\n",
        "            print((index, prediction))\n",
        "\n",
        "\n",
        "\n",
        "            if len(results_list) % 50 == 0:\n",
        "                save_csv_to_staging_area(model_name=model_name,\n",
        "                                         dataset_name=key,\n",
        "                                         results_list=results_list)\n",
        "\n",
        "            rate_limits = make_rate_limits_request().json()\n",
        "            rate_limit = rate_limits.get(\"data\").get(\"rate_limit\").get(\"requests\")\n",
        "\n",
        "            if rate_limits == 0:\n",
        "                raise ValueError(\"Rate limit exceed\")\n",
        "\n",
        "\n",
        "        save_csv_to_staging_area(model_name=model_name,\n",
        "                                 dataset_name=key,\n",
        "                                 results_list=results_list)\n",
        "\n",
        "\n",
        "\n",
        "        if len(results_list) != (dataset['df'].shape[0]):\n",
        "            df = convert_csv_in_staging_area_to_dataframe(get_staging_area_files_by_model_or_dataset_name(dataset_name=key, model_name=model_name))\n",
        "            if df.shape[0] == dataset['df'].shape[0]:\n",
        "                dataset['df']['predictions'] = df['predictions']\n",
        "                dataset['df']['reasoning'] = df['reasoning']\n",
        "                dataset['df']['provider'] = df['provider']\n",
        "                dataset['df']['finish_reason'] = df['finish_reason']\n",
        "                dataset['df']['prompt_tokens'] = df['prompt_tokens']\n",
        "                dataset['df']['completion_tokens'] = df['completion_tokens']\n",
        "                dataset['df']['total_tokens'] = df['total_tokens']\n",
        "\n",
        "\n",
        "        elif len(results_list) == (dataset['df'].shape[0]):\n",
        "            dataset['df']['predictions'] = np.array(results_list)[:,1]\n",
        "            dataset['df']['reasoning'] = np.array(results_list)[:,2]\n",
        "            dataset['df']['provider'] = np.array(results_list)[:,3]\n",
        "            dataset['df']['finish_reason'] = np.array(results_list)[:,4]\n",
        "            dataset['df']['prompt_tokens'] = np.array(results_list)[:,5]\n",
        "            dataset['df']['completion_tokens'] = np.array(results_list)[:,6]\n",
        "            dataset['df']['total_tokens'] = np.array(results_list)[:,7]\n",
        "\n",
        "\n",
        "        else:\n",
        "            raise ValueError('The number of results is different from the number of rows in the dataset.')\n",
        "\n",
        "        dataset['df'].to_csv(f'{str(dataset[\"path\"])}/{key}_{MODEL_NAME}_result_v2.csv', index=False)\n",
        "        RESULT_DATASETS = [Path(dataset).stem.split(f\"_{MODEL_NAME}_result_v2\")[0] for dataset in glob.glob(f'{PREPROCESSED_DATA_ROOT_PATH}/*/*_{MODEL_NAME}_result_v2.csv')]\n",
        "        print(f'The evaluation of dataset:{key} has ended.')"
      ],
      "metadata": {
        "id": "imcx4oTjZi1m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}